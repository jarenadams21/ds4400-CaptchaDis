{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile as wv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm # progress bar\n",
    "from audioMNIST import AudioMNIST\n",
    "from DClassifier import CNN2DAudioClassifier\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from kNN import KNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA device\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, val_dl): \n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "        \n",
    "    for data in tqdm(val_dl):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, prediction = torch.max(outputs, 1)\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "    num_batches = len(val_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction / total_prediction\n",
    "\n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def training(model, train_dl, val_dl, num_epochs, \n",
    "             criterion, optimizer, scheduler):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        for data in tqdm(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        \n",
    "        v_acc, v_loss = evaluate(model.to(device), val_dl)\n",
    "        \n",
    "        print(\"Epoch: %d, Loss: %.4f, Train Accuracy: %.2f, Val. Loss: %.4f, Val. Accuracy: %.2f\" % (\n",
    "            epoch + 1, avg_loss, acc, v_loss, v_acc\n",
    "        ))\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        val_losses.append(v_loss)\n",
    "        \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = AudioMNIST()\n",
    "    train_dl = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    model = CNN2DAudioClassifier().to(device)\n",
    "    from torch.utils.data import random_split\n",
    "\n",
    "    n_items = len(dataset)\n",
    "\n",
    "    # items for train, validation, and test sets\n",
    "    n_train = round(n_items * 0.7)\n",
    "    n_val = round(n_items * 0.15)\n",
    "     ## Ensures that rounding errors don't leave any data out\n",
    "    n_test = n_items - n_train - n_val\n",
    "\n",
    "    # split the dataset into training, validation, and test sets\n",
    "    train_ds, val_test_ds = random_split(dataset, [n_train, n_items - n_train])\n",
    "    val_ds, test_ds = random_split(val_test_ds, [n_val, n_test])\n",
    "\n",
    "    # create DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
    "    test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioMNIST()\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the AudioMNIST dataset and DataLoader\n",
    "dataset = AudioMNIST()\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=True, num_workers=4)\n",
    "\n",
    "# Load all data into memory\n",
    "all_features = []\n",
    "all_labels = []\n",
    "for data, target in loader:\n",
    "    all_features.append(data.numpy())  # Assuming data is already a tensor\n",
    "    all_labels.append(target.numpy())\n",
    "\n",
    "# Convert lists to numpy arrays and reshape if necessary\n",
    "all_features = np.vstack(all_features)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Flatten features if necessary (depends on mfcc shape)\n",
    "all_features = all_features.reshape(all_features.shape[0], -1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize and train kNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy on the test set\n",
    "predictions = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_COUNT = 4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=0.001, \n",
    "                                                steps_per_epoch=int(len(train_dl)), \n",
    "                                                epochs=EPOCH_COUNT, \n",
    "                                                anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, val_losses = training(model, train_dl, val_dl, EPOCH_COUNT, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to use in Gradio App\n",
    "#torch.save(model.state_dict(), 'audio_classifier_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d33a0debf81d76012e3fb50f9daecad1e12ff320a017e9164c9b4ad368a7e6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('space': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
